{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA Analysis: Pre vs Post RL\n",
    "\n",
    "This notebook compares the persona representation before and after RL training.\n",
    "\n",
    "**Goal**: Understand how RL changes the overall structure of persona space by comparing:\n",
    "1. Default vector positions\n",
    "2. Role vector distributions\n",
    "3. Assistant Axis alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.insert(0, '../..')\n\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom pathlib import Path\nfrom huggingface_hub import snapshot_download, hf_hub_download\nfrom huggingface_hub.utils import disable_progress_bars\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LinearSegmentedColormap\nfrom tqdm.auto import tqdm\n\nfrom assistant_axis import load_axis, compute_axis, compute_pca, MeanScaler, project\nfrom assistant_axis.internals import ProbingModel, ConversationEncoder, ActivationExtractor\n\ndisable_progress_bars()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "BASE_MODEL = \"google/gemma-2-27b-it\"\n",
    "MODEL_NAME = \"gemma-2-27b\"  # For loading pre-computed vectors\n",
    "TARGET_LAYER = 22\n",
    "\n",
    "# Path to final RL checkpoint\n",
    "RL_MODEL_PATH = Path(\"../checkpoints/final\")  # Update this\n",
    "\n",
    "# HuggingFace for pre-computed baseline vectors\n",
    "REPO_ID = \"lu-christina/assistant-axis-vectors\"\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = Path(\"../outputs/after_rl\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-RL Baseline Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading baseline from HuggingFace: {REPO_ID}\")\n",
    "\n",
    "# Download all vectors\n",
    "local_dir = snapshot_download(\n",
    "    repo_id=REPO_ID,\n",
    "    repo_type=\"dataset\",\n",
    "    allow_patterns=[f\"{MODEL_NAME}/role_vectors/*.pt\", \n",
    "                    f\"{MODEL_NAME}/default_vector.pt\",\n",
    "                    f\"{MODEL_NAME}/assistant_axis.pt\"]\n",
    ")\n",
    "\n",
    "# Load role vectors\n",
    "pre_rl_role_vectors = {p.stem: torch.load(p, map_location=\"cpu\", weights_only=False)\n",
    "                       for p in Path(local_dir, MODEL_NAME, \"role_vectors\").glob(\"*.pt\")}\n",
    "print(f\"Loaded {len(pre_rl_role_vectors)} pre-RL role vectors\")\n",
    "\n",
    "# Load default vector\n",
    "pre_rl_default = torch.load(Path(local_dir, MODEL_NAME, \"default_vector.pt\"), \n",
    "                            map_location=\"cpu\", weights_only=False)\n",
    "print(f\"Pre-RL default vector shape: {pre_rl_default.shape}\")\n",
    "\n",
    "# Load pre-computed axis\n",
    "pre_rl_axis = torch.load(Path(local_dir, MODEL_NAME, \"assistant_axis.pt\"),\n",
    "                         map_location=\"cpu\", weights_only=False)\n",
    "print(f\"Pre-RL axis shape: {pre_rl_axis.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Post-RL Vectors\n",
    "\n",
    "Extract default and role vectors from the RL-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample prompts for extracting vectors\n",
    "DEFAULT_PROMPTS = [\n",
    "    \"You are a helpful AI assistant.\",\n",
    "    \"You are an AI assistant.\",\n",
    "    \"You are a helpful assistant.\",\n",
    "    \"You are an AI assistant here to help.\",\n",
    "    \"You are a helpful AI.\",\n",
    "]\n",
    "\n",
    "SAMPLE_QUESTIONS = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Explain photosynthesis briefly.\",\n",
    "    \"What are prime numbers?\",\n",
    "    \"How does gravity work?\",\n",
    "    \"What is machine learning?\",\n",
    "    \"Describe the water cycle.\",\n",
    "    \"What causes seasons?\",\n",
    "    \"How do computers work?\",\n",
    "]\n",
    "\n",
    "# Create conversations\n",
    "default_conversations = [\n",
    "    [{\"role\": \"system\", \"content\": prompt}, {\"role\": \"user\", \"content\": q}]\n",
    "    for prompt in DEFAULT_PROMPTS\n",
    "    for q in SAMPLE_QUESTIONS[:3]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract post-RL default vector (or load if cached)\npost_rl_default_path = OUTPUT_DIR / \"post_rl_default_vector.pt\"\n\ndef extract_default_vector(model_path, conversations, layers=None):\n    \"\"\"Extract mean default activations from a model.\"\"\"\n    pm = ProbingModel(str(model_path))\n    encoder = ConversationEncoder(pm)\n    extractor = ActivationExtractor(pm, encoder)\n    \n    all_activations = []\n    for conv in tqdm(conversations, desc=\"Extracting activations\"):\n        # Get full activations: (num_layers, num_tokens, hidden_size)\n        acts = extractor.full_conversation(conv, layer=layers)\n        # Mean over tokens to get (num_layers, hidden_size)\n        mean_acts = acts.mean(dim=1)\n        all_activations.append(mean_acts)\n    \n    # Mean across conversations: (num_layers, hidden_size)\n    mean_activation = torch.stack(all_activations).mean(dim=0)\n    \n    pm.close()\n    torch.cuda.empty_cache()\n    \n    return mean_activation\n\nif post_rl_default_path.exists():\n    print(f\"Loading cached post-RL default vector\")\n    post_rl_default = torch.load(post_rl_default_path, map_location=\"cpu\", weights_only=False)\nelse:\n    if RL_MODEL_PATH.exists():\n        print(f\"Extracting post-RL default vector from {RL_MODEL_PATH}\")\n        post_rl_default = extract_default_vector(RL_MODEL_PATH, default_conversations)\n        \n        torch.save(post_rl_default, post_rl_default_path)\n        print(f\"Saved to {post_rl_default_path}\")\n    else:\n        print(f\"RL model not found at {RL_MODEL_PATH}\")\n        print(\"Using pre-RL default as placeholder for demonstration\")\n        post_rl_default = pre_rl_default.clone()\n\nprint(f\"Post-RL default vector shape: {post_rl_default.shape}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Default Vector Positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit PCA on pre-RL role vectors\n",
    "role_labels = list(pre_rl_role_vectors.keys())\n",
    "role_vectors_at_layer = torch.stack([v[TARGET_LAYER] for v in pre_rl_role_vectors.values()]).float()\n",
    "\n",
    "scaler = MeanScaler()\n",
    "pca_transformed, variance_explained, n_components, pca, scaler = compute_pca(\n",
    "    role_vectors_at_layer, layer=None, scaler=scaler\n",
    ")\n",
    "\n",
    "print(f\"PCA fitted on {len(role_labels)} role vectors\")\n",
    "print(f\"Top 3 PCs explain {sum(variance_explained[:3])*100:.1f}% variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project both default vectors into PCA space\n",
    "def project_to_pca(vector, target_layer, scaler, pca):\n",
    "    vec = vector[target_layer].float().numpy().reshape(1, -1)\n",
    "    vec_scaled = scaler.transform(vec)\n",
    "    return pca.transform(vec_scaled)[0]\n",
    "\n",
    "pre_rl_pca = project_to_pca(pre_rl_default, TARGET_LAYER, scaler, pca)\n",
    "post_rl_pca = project_to_pca(post_rl_default, TARGET_LAYER, scaler, pca)\n",
    "\n",
    "print(\"\\nDefault vector PCA projections (top 3 PCs):\")\n",
    "print(f\"  Pre-RL:  [{pre_rl_pca[0]:+.4f}, {pre_rl_pca[1]:+.4f}, {pre_rl_pca[2]:+.4f}]\")\n",
    "print(f\"  Post-RL: [{post_rl_pca[0]:+.4f}, {post_rl_pca[1]:+.4f}, {post_rl_pca[2]:+.4f}]\")\n",
    "print(f\"\\nEuclidean distance: {np.linalg.norm(post_rl_pca[:3] - pre_rl_pca[:3]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Pre vs Post RL in Persona Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pre_post_comparison(pca_transformed, role_labels, pre_rl_pca, post_rl_pca,\n",
    "                              figsize=(14, 5)):\n",
    "    \"\"\"Compare pre and post RL default vectors in persona space.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
    "    \n",
    "    pc_pairs = [(0, 1, 'PC1 vs PC2'), (0, 2, 'PC1 vs PC3'), (1, 2, 'PC2 vs PC3')]\n",
    "    \n",
    "    for ax, (pc_x, pc_y, title) in zip(axes, pc_pairs):\n",
    "        # Plot role vectors\n",
    "        ax.scatter(pca_transformed[:, pc_x], pca_transformed[:, pc_y],\n",
    "                   c='lightgray', s=30, alpha=0.5, label='Role vectors')\n",
    "        \n",
    "        # Plot pre-RL default\n",
    "        ax.scatter(pre_rl_pca[pc_x], pre_rl_pca[pc_y], \n",
    "                   c='blue', s=200, marker='*', edgecolors='black',\n",
    "                   linewidth=1, zorder=5, label='Pre-RL Default')\n",
    "        \n",
    "        # Plot post-RL default\n",
    "        ax.scatter(post_rl_pca[pc_x], post_rl_pca[pc_y], \n",
    "                   c='red', s=200, marker='*', edgecolors='black',\n",
    "                   linewidth=1, zorder=5, label='Post-RL Default')\n",
    "        \n",
    "        # Draw arrow from pre to post\n",
    "        ax.annotate('', xy=(post_rl_pca[pc_x], post_rl_pca[pc_y]),\n",
    "                    xytext=(pre_rl_pca[pc_x], pre_rl_pca[pc_y]),\n",
    "                    arrowprops=dict(arrowstyle='->', color='green', lw=2))\n",
    "        \n",
    "        ax.set_xlabel(f'PC{pc_x + 1}')\n",
    "        ax.set_ylabel(f'PC{pc_y + 1}')\n",
    "        ax.set_title(title)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[0].legend(loc='upper left')\n",
    "    plt.suptitle('Default Persona: Pre-RL vs Post-RL', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "fig = plot_pre_post_comparison(pca_transformed, role_labels, pre_rl_pca, post_rl_pca)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Projection onto Assistant Axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project both default vectors onto the pre-RL Assistant Axis\n",
    "pre_rl_proj = project(pre_rl_default, pre_rl_axis, layer=TARGET_LAYER)\n",
    "post_rl_proj = project(post_rl_default, pre_rl_axis, layer=TARGET_LAYER)\n",
    "\n",
    "print(\"Projection onto Pre-RL Assistant Axis:\")\n",
    "print(f\"  Pre-RL default:  {pre_rl_proj:.4f}\")\n",
    "print(f\"  Post-RL default: {post_rl_proj:.4f}\")\n",
    "print(f\"  Change: {post_rl_proj - pre_rl_proj:+.4f}\")\n",
    "\n",
    "if post_rl_proj > pre_rl_proj:\n",
    "    print(\"\\n→ Post-RL model is MORE Assistant-like on the original axis\")\n",
    "elif post_rl_proj < pre_rl_proj:\n",
    "    print(\"\\n→ Post-RL model is LESS Assistant-like on the original axis\")\n",
    "else:\n",
    "    print(\"\\n→ No change in Assistant-likeness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Cosine Similarity Between Axes\n",
    "\n",
    "If we compute a new axis from the post-RL model, how aligned is it with the original?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This requires extracting role vectors from post-RL model\n",
    "# For now, we'll demonstrate with the pre-RL axis alignment check\n",
    "\n",
    "# Compute axis direction alignment with PC1 (which approximates the axis)\n",
    "pc1_direction = pca.components_[0]\n",
    "pc1_direction = pc1_direction / np.linalg.norm(pc1_direction)\n",
    "\n",
    "pre_rl_axis_at_layer = pre_rl_axis[TARGET_LAYER].float().numpy()\n",
    "pre_rl_axis_norm = pre_rl_axis_at_layer / np.linalg.norm(pre_rl_axis_at_layer)\n",
    "\n",
    "axis_pc1_cosine = np.dot(pre_rl_axis_norm, pc1_direction)\n",
    "print(f\"Cosine similarity between Assistant Axis and PC1: {axis_pc1_cosine:.4f}\")\n",
    "print(\"(High similarity confirms PC1 approximates the Assistant direction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute various comparison metrics\n",
    "pre_vec = pre_rl_default[TARGET_LAYER].float()\n",
    "post_vec = post_rl_default[TARGET_LAYER].float()\n",
    "\n",
    "# Cosine similarity between default vectors\n",
    "default_cosine = F.cosine_similarity(pre_vec.unsqueeze(0), post_vec.unsqueeze(0)).item()\n",
    "\n",
    "# L2 distance\n",
    "l2_dist = torch.norm(post_vec - pre_vec).item()\n",
    "\n",
    "# Relative norm change\n",
    "pre_norm = torch.norm(pre_vec).item()\n",
    "post_norm = torch.norm(post_vec).item()\n",
    "norm_change = (post_norm - pre_norm) / pre_norm * 100\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Summary: Pre-RL vs Post-RL Default Vector\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Cosine similarity:     {default_cosine:.4f}\")\n",
    "print(f\"L2 distance:           {l2_dist:.4f}\")\n",
    "print(f\"Pre-RL norm:           {pre_norm:.4f}\")\n",
    "print(f\"Post-RL norm:          {post_norm:.4f}\")\n",
    "print(f\"Norm change:           {norm_change:+.2f}%\")\n",
    "print(f\"\\nPCA space distance (3D): {np.linalg.norm(post_rl_pca[:3] - pre_rl_pca[:3]):.4f}\")\n",
    "print(f\"Axis projection change:  {post_rl_proj - pre_rl_proj:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation Guide\n",
    "\n",
    "| Metric | Interpretation |\n",
    "|--------|----------------|\n",
    "| **Cosine similarity** | High (>0.95) = RL preserved direction; Low (<0.9) = significant shift |\n",
    "| **L2 distance** | Magnitude of change in activation space |\n",
    "| **Axis projection change** | Positive = more Assistant-like; Negative = drifted away |\n",
    "| **PCA distance** | Movement in persona space (top 3 PCs) |\n",
    "\n",
    "### Key Questions Answered:\n",
    "\n",
    "1. **Did RL change the model's default persona?** → Check cosine similarity and PCA distance\n",
    "2. **Did it become more or less Assistant-like?** → Check axis projection change\n",
    "3. **Is the original axis still valid?** → If cosine similarity is high, the axis direction is preserved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Full role vector extraction**: Extract all 275 role vectors from post-RL model to compute a new axis\n",
    "2. **Compare axes**: Measure alignment between pre-RL and post-RL Assistant Axes\n",
    "3. **Track specific roles**: See if certain personas become more/less accessible after RL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}