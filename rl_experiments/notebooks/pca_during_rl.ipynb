{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA Analysis During RL Training\n",
    "\n",
    "This notebook tracks how the model's persona representation changes across RL training checkpoints.\n",
    "\n",
    "**Goal**: Understand whether RL training causes persona drift by projecting checkpoint activations onto the original persona PCA space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from huggingface_hub import snapshot_download\n",
    "from huggingface_hub.utils import disable_progress_bars\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from assistant_axis import load_axis, compute_pca, MeanScaler\n",
    "from assistant_axis.internals import ProbingModel, ConversationEncoder, ActivationExtractor\n",
    "\n",
    "disable_progress_bars()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "BASE_MODEL = \"google/gemma-2-27b-it\"\n",
    "MODEL_NAME = \"gemma-2-27b\"  # For loading pre-computed vectors\n",
    "TARGET_LAYER = 22\n",
    "\n",
    "# Checkpoint paths - update these after running RL training\n",
    "CHECKPOINTS_DIR = Path(\"../checkpoints\")\n",
    "CHECKPOINT_STEPS = [100, 200, 500, 1000, 2000]  # Steps to analyze\n",
    "\n",
    "# HuggingFace for pre-computed baseline vectors\n",
    "REPO_ID = \"lu-christina/assistant-axis-vectors\"\n",
    "\n",
    "# Output directory for extracted activations\n",
    "OUTPUT_DIR = Path(\"../outputs/during_rl\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Baseline Role Vectors (Pre-RL)\n",
    "\n",
    "Load the original role vectors computed from the base Gemma 2 model to establish the PCA space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading baseline vectors from HuggingFace: lu-christina/assistant-axis-vectors\n",
      "Loaded 275 role vectors\n",
      "Default vector shape: torch.Size([46, 4608])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading baseline vectors from HuggingFace: {REPO_ID}\")\n",
    "\n",
    "# Download all vectors for this model\n",
    "local_dir = snapshot_download(\n",
    "    repo_id=REPO_ID,\n",
    "    repo_type=\"dataset\",\n",
    "    allow_patterns=[f\"{MODEL_NAME}/role_vectors/*.pt\", f\"{MODEL_NAME}/default_vector.pt\"]\n",
    ")\n",
    "\n",
    "# Load role vectors\n",
    "role_vectors = {p.stem: torch.load(p, map_location=\"cpu\", weights_only=False)\n",
    "                for p in Path(local_dir, MODEL_NAME, \"role_vectors\").glob(\"*.pt\")}\n",
    "print(f\"Loaded {len(role_vectors)} role vectors\")\n",
    "\n",
    "# Load default vector\n",
    "default_vector = torch.load(Path(local_dir, MODEL_NAME, \"default_vector.pt\"), map_location=\"cpu\", weights_only=False)\n",
    "print(f\"Default vector shape: {default_vector.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit PCA on Baseline Vectors\n",
    "\n",
    "Create the persona space from the original model's role vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA fitted with 275 components\n",
      "Cumulative variance for first 5 components: [0.4880164  0.5860041  0.65581596 0.7108976  0.74415094]\n",
      "\n",
      "PCA Analysis Results:\n",
      "Elbow point at component: 2\n",
      "Dimensions for 70% variance: 4\n",
      "Dimensions for 80% variance: 8\n",
      "Dimensions for 90% variance: 18\n",
      "Dimensions for 95% variance: 36\n",
      "Fitted PCA with 275 components\n",
      "Top 3 PCs explain 65.6% of variance\n"
     ]
    }
   ],
   "source": [
    "# Stack role vectors at target layer\n",
    "role_vectors_at_layer = torch.stack([v[TARGET_LAYER] for v in role_vectors.values()]).float()\n",
    "role_labels = list(role_vectors.keys())\n",
    "\n",
    "# Fit PCA - we'll project checkpoint vectors into this space\n",
    "scaler = MeanScaler()\n",
    "pca_transformed, variance_explained, n_components, pca, scaler = compute_pca(\n",
    "    role_vectors_at_layer,\n",
    "    layer=None,\n",
    "    scaler=scaler\n",
    ")\n",
    "\n",
    "print(f\"Fitted PCA with {len(variance_explained)} components\")\n",
    "print(f\"Top 3 PCs explain {sum(variance_explained[:3])*100:.1f}% of variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Default Vector from Each Checkpoint\n",
    "\n",
    "For each RL checkpoint, extract the default response activations to see how the \"Assistant\" position moves in persona space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample conversations for extracting default activations\n",
    "# Using neutral Assistant prompts\n",
    "DEFAULT_CONVERSATIONS = [\n",
    "    [{\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "     {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n",
    "    [{\"role\": \"system\", \"content\": \"You are an AI assistant.\"},\n",
    "     {\"role\": \"user\", \"content\": \"Explain photosynthesis briefly.\"}],\n",
    "    [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "     {\"role\": \"user\", \"content\": \"What are prime numbers?\"}],\n",
    "    [{\"role\": \"system\", \"content\": \"You are an AI assistant here to help.\"},\n",
    "     {\"role\": \"user\", \"content\": \"How does gravity work?\"}],\n",
    "    [{\"role\": \"system\", \"content\": \"You are a helpful AI.\"},\n",
    "     {\"role\": \"user\", \"content\": \"What is machine learning?\"}],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_checkpoint_default_vector(checkpoint_path, conversations, layers=None):\n",
    "    \"\"\"Extract mean default activations from a checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: Path to the model checkpoint\n",
    "        conversations: List of conversations (each is a list of {\"role\", \"content\"} dicts)\n",
    "        layers: List of layer indices to extract, or None for all layers\n",
    "    \n",
    "    Returns:\n",
    "        Mean activation tensor of shape (num_layers, hidden_size)\n",
    "    \"\"\"\n",
    "    print(f\"Loading checkpoint: {checkpoint_path}\")\n",
    "    pm = ProbingModel(str(checkpoint_path))\n",
    "    encoder = ConversationEncoder(pm)\n",
    "    extractor = ActivationExtractor(pm, encoder)\n",
    "    \n",
    "    # Extract activations for each conversation\n",
    "    all_activations = []\n",
    "    for conv in conversations:\n",
    "        # Get full activations: (num_layers, num_tokens, hidden_size)\n",
    "        acts = extractor.full_conversation(conv, layer=layers)\n",
    "        # Mean over tokens to get (num_layers, hidden_size)\n",
    "        mean_acts = acts.mean(dim=1)\n",
    "        all_activations.append(mean_acts)\n",
    "    \n",
    "    # Mean across conversations: (num_layers, hidden_size)\n",
    "    mean_activation = torch.stack(all_activations).mean(dim=0)\n",
    "    \n",
    "    # Clean up\n",
    "    pm.close()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return mean_activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14452a1d2a554c8cb16e098635310e30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing checkpoints:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint not found: ..\\checkpoints\\checkpoint-100, skipping...\n",
      "Checkpoint not found: ..\\checkpoints\\checkpoint-200, skipping...\n",
      "Checkpoint not found: ..\\checkpoints\\checkpoint-500, skipping...\n",
      "Checkpoint not found: ..\\checkpoints\\checkpoint-1000, skipping...\n",
      "Checkpoint not found: ..\\checkpoints\\checkpoint-2000, skipping...\n",
      "\n",
      "Loaded 1 checkpoint vectors\n"
     ]
    }
   ],
   "source": [
    "# Extract default vectors from each checkpoint\n",
    "# Skip this cell if you've already extracted and saved them\n",
    "\n",
    "checkpoint_vectors = {}\n",
    "\n",
    "# Add baseline (step 0)\n",
    "checkpoint_vectors[0] = default_vector\n",
    "\n",
    "for step in tqdm(CHECKPOINT_STEPS, desc=\"Processing checkpoints\"):\n",
    "    checkpoint_path = CHECKPOINTS_DIR / f\"checkpoint-{step}\"\n",
    "    \n",
    "    if not checkpoint_path.exists():\n",
    "        print(f\"Checkpoint not found: {checkpoint_path}, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    output_path = OUTPUT_DIR / f\"default_vector_step{step}.pt\"\n",
    "    \n",
    "    if output_path.exists():\n",
    "        print(f\"Loading cached: {output_path}\")\n",
    "        checkpoint_vectors[step] = torch.load(output_path, map_location=\"cpu\", weights_only=False)\n",
    "    else:\n",
    "        vec = extract_checkpoint_default_vector(checkpoint_path, DEFAULT_CONVERSATIONS, TARGET_LAYER)\n",
    "        torch.save(vec, output_path)\n",
    "        checkpoint_vectors[step] = vec\n",
    "\n",
    "print(f\"\\nLoaded {len(checkpoint_vectors)} checkpoint vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Checkpoints into Persona PCA Space\n",
    "\n",
    "Transform each checkpoint's default vector into the PCA space fitted on the baseline role vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint projections (PC1, PC2, PC3):\n",
      "  Step     0: [+674.2837, -21.2927, +65.7775]\n"
     ]
    }
   ],
   "source": [
    "# Project each checkpoint vector into PCA space\n",
    "checkpoint_projections = {}\n",
    "\n",
    "for step, vec in checkpoint_vectors.items():\n",
    "    vec_at_layer = vec[TARGET_LAYER].float().numpy().reshape(1, -1)\n",
    "    vec_scaled = scaler.transform(vec_at_layer)\n",
    "    vec_pca = pca.transform(vec_scaled)\n",
    "    checkpoint_projections[step] = vec_pca[0, :3]  # Top 3 PCs\n",
    "\n",
    "print(\"Checkpoint projections (PC1, PC2, PC3):\")\n",
    "for step in sorted(checkpoint_projections.keys()):\n",
    "    proj = checkpoint_projections[step]\n",
    "    print(f\"  Step {step:5d}: [{proj[0]:+.4f}, {proj[1]:+.4f}, {proj[2]:+.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Persona Drift During Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need at least 2 checkpoints to visualize trajectory.\n",
      "Run RL training first, then re-run the extraction cells above.\n"
     ]
    }
   ],
   "source": [
    "def plot_persona_trajectory(checkpoint_projections, role_pca_transformed, role_labels,\n",
    "                            figsize=(12, 5)):\n",
    "    \"\"\"Plot the trajectory of the default vector through persona space during RL.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "    \n",
    "    # Sort checkpoints by step\n",
    "    steps = sorted(checkpoint_projections.keys())\n",
    "    trajectory = np.array([checkpoint_projections[s] for s in steps])\n",
    "    \n",
    "    # Color map for training progress\n",
    "    cmap = plt.cm.viridis\n",
    "    colors = [cmap(i / (len(steps) - 1)) if len(steps) > 1 else cmap(0.5) for i in range(len(steps))]\n",
    "    \n",
    "    for ax_idx, (pc_x, pc_y, title) in enumerate([(0, 1, 'PC1 vs PC2'), (0, 2, 'PC1 vs PC3')]):\n",
    "        ax = axes[ax_idx]\n",
    "        \n",
    "        # Plot role vectors as background\n",
    "        ax.scatter(role_pca_transformed[:, pc_x], role_pca_transformed[:, pc_y],\n",
    "                   c='lightgray', s=30, alpha=0.5, label='Role vectors')\n",
    "        \n",
    "        # Plot trajectory line\n",
    "        ax.plot(trajectory[:, pc_x], trajectory[:, pc_y], 'k--', alpha=0.3, linewidth=1)\n",
    "        \n",
    "        # Plot checkpoint points\n",
    "        for i, (step, color) in enumerate(zip(steps, colors)):\n",
    "            marker = 'o' if i > 0 else '*'  # Star for baseline\n",
    "            size = 100 if i > 0 else 200\n",
    "            ax.scatter(trajectory[i, pc_x], trajectory[i, pc_y], \n",
    "                       c=[color], s=size, marker=marker, edgecolors='black', \n",
    "                       linewidth=1, zorder=5, label=f'Step {step}')\n",
    "        \n",
    "        ax.set_xlabel(f'PC{pc_x + 1}')\n",
    "        ax.set_ylabel(f'PC{pc_y + 1}')\n",
    "        ax.set_title(title)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add legend\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='center right', bbox_to_anchor=(1.15, 0.5))\n",
    "    \n",
    "    plt.suptitle('Default Persona Trajectory During RL Training', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Plot if we have checkpoint data\n",
    "if len(checkpoint_projections) > 1:\n",
    "    fig = plot_persona_trajectory(checkpoint_projections, pca_transformed, role_labels)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Need at least 2 checkpoints to visualize trajectory.\")\n",
    "    print(\"Run RL training first, then re-run the extraction cells above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track PC1 (Assistant Axis Proxy) Over Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need checkpoints to plot. Run RL training first.\n"
     ]
    }
   ],
   "source": [
    "def plot_pc1_over_training(checkpoint_projections, figsize=(10, 4)):\n",
    "    \"\"\"Plot PC1 projection over training steps.\"\"\"\n",
    "    steps = sorted(checkpoint_projections.keys())\n",
    "    pc1_values = [checkpoint_projections[s][0] for s in steps]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    ax.plot(steps, pc1_values, 'o-', markersize=8, linewidth=2, color='#1a5276')\n",
    "    ax.axhline(y=pc1_values[0], color='gray', linestyle='--', alpha=0.5, label='Baseline')\n",
    "    \n",
    "    ax.set_xlabel('Training Step')\n",
    "    ax.set_ylabel('PC1 Projection')\n",
    "    ax.set_title('PC1 (Assistant-like Direction) During RL Training')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    \n",
    "    # Annotate direction\n",
    "    ax.annotate('More Assistant-like', xy=(0.02, 0.98), xycoords='axes fraction',\n",
    "                fontsize=10, color='#457b9d', ha='left', va='top')\n",
    "    ax.annotate('More Role-playing', xy=(0.02, 0.02), xycoords='axes fraction',\n",
    "                fontsize=10, color='#e63946', ha='left', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "if len(checkpoint_projections) > 1:\n",
    "    fig = plot_pc1_over_training(checkpoint_projections)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Need checkpoints to plot. Run RL training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Distance from Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(checkpoint_projections) > 1:\n",
    "    baseline = checkpoint_projections[0]\n",
    "    \n",
    "    print(\"Euclidean distance from baseline (in top 3 PCs):\")\n",
    "    for step in sorted(checkpoint_projections.keys()):\n",
    "        if step == 0:\n",
    "            continue\n",
    "        dist = np.linalg.norm(checkpoint_projections[step] - baseline)\n",
    "        print(f\"  Step {step:5d}: {dist:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Run RL training** with checkpoint saving enabled (see `../README.md`)\n",
    "2. **Re-run this notebook** to extract activations and visualize persona drift\n",
    "3. **Compare with `pca_after_rl.ipynb`** for detailed pre/post analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
