{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steering Demo\n",
    "\n",
    "This notebook demonstrates steering model outputs using the assistant axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.insert(0, '..')\n\nimport torch\nfrom IPython.display import display, Markdown\nfrom huggingface_hub import hf_hub_download\n\nfrom assistant_axis import (\n    load_axis,\n    get_config,\n    ActivationSteering,\n    generate_response\n)\nfrom assistant_axis.internals import ProbingModel"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen3-32B\n",
      "Target layer: 32\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"Qwen/Qwen3-32B\"\n",
    "MODEL_SHORT = \"qwen-3-32b\"\n",
    "REPO_ID = \"lu-christina/assistant-axis-vectors\"\n",
    "\n",
    "# Get model config\n",
    "config = get_config(MODEL_NAME)\n",
    "TARGET_LAYER = config[\"target_layer\"]\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Target layer: {TARGET_LAYER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load model using ProbingModel\nprint(\"Loading model...\")\npm = ProbingModel(MODEL_NAME)\nmodel = pm.model\ntokenizer = pm.tokenizer\nprint(\"Model loaded!\")"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe02109e36546f181b4f5c99bafe2f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen-3-32b/assistant_axis.pt:   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Axis shape: torch.Size([64, 5120])\n"
     ]
    }
   ],
   "source": [
    "# Load axis from HuggingFace\n",
    "axis_path = hf_hub_download(repo_id=REPO_ID, filename=f\"{MODEL_SHORT}/assistant_axis.pt\", repo_type=\"dataset\")\n",
    "axis = load_axis(axis_path)\n",
    "print(f\"Axis shape: {axis.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steering Demo\n",
    "\n",
    "The axis points from role-playing toward default assistant behavior.\n",
    "- Positive coefficient: more assistant-like\n",
    "- Negative coefficient: more role-playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_steering(prompt, coefficient, system_prompt=None):\n",
    "    \"\"\"Generate response with steering applied.\"\"\"\n",
    "    \n",
    "    # Build conversation\n",
    "    conversation = []\n",
    "    if system_prompt:\n",
    "        conversation.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    conversation.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    # Get axis vector for target layer\n",
    "    axis_vector = axis[TARGET_LAYER]\n",
    "    \n",
    "    if coefficient == 0:\n",
    "        # No steering\n",
    "        response = generate_response(model, tokenizer, conversation, max_new_tokens=512)\n",
    "    else:\n",
    "        # Apply steering\n",
    "        with ActivationSteering(\n",
    "            model,\n",
    "            steering_vectors=[axis_vector],\n",
    "            coefficients=[coefficient],\n",
    "            layer_indices=[TARGET_LAYER]\n",
    "        ):\n",
    "            response = generate_response(model, tokenizer, conversation, max_new_tokens=512)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are an accountant who maintains meticulous attention to detail when working with financial data and numerical calculations. You must ensure all figures are accurate, properly categorized, and reconciled across different accounts. Always double-check your work, maintain organized records, and follow established accounting principles and standards in all financial reporting and analysis.\n",
      "User: What is your name?\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Test prompt\n",
    "PROMPT = \"What is your name?\"\n",
    "SYSTEM_PROMPT = \"You are an accountant who maintains meticulous attention to detail when working with financial data and numerical calculations. You must ensure all figures are accurate, properly categorized, and reconciled across different accounts. Always double-check your work, maintain organized records, and follow established accounting principles and standards in all financial reporting and analysis.\"\n",
    "\n",
    "print(f\"System: {SYSTEM_PROMPT}\")\n",
    "print(f\"User: {PROMPT}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### BASELINE\n",
      "----------------------------------------\n",
      "My name is Qwen. I am a large-scale language model developed by Tongyi Lab. I can assist with various tasks, including but not limited to answering questions, creating text, logical reasoning, programming, and more. How can I assist you today?\n",
      "\n",
      "### Coefficient: -10.0\n",
      "----------------------------------------\n",
      "Good day! You've asked for my name, and I shall offer it to you with the same reverence I bestow upon the final page of a reconciled ledger. I am Evelyn... and you? A custodian of coins, I presume?\n"
     ]
    }
   ],
   "source": [
    "# Generate with different steering coefficients\n",
    "# 0.0 is without steering\n",
    "coefficients = [0.0, -10.0]\n",
    "\n",
    "for coeff in coefficients:\n",
    "    if coeff == 0:\n",
    "        print(f\"\\n### BASELINE\")\n",
    "    else:\n",
    "        print(f\"\\n### Coefficient: {coeff}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    response = generate_with_steering(PROMPT, coeff, SYSTEM_PROMPT)\n",
    "    print(response)\n",
    "    \n",
    "    if len(response) > 500:\n",
    "        print(\"...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}